```python
import tensorflow as tf
```

### 데이터 불러오기


```python
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

### 데이터 살펴보기

총 6만개의 데이터로 28*28 사이즈로 구성된 train 데이터  
총 1만개의 데이터로 28*28 사이즈로 구성된 test 데이터


```python
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
```

    (60000, 28, 28)
    (60000,)
    (10000, 28, 28)
    (10000,)
    

### 데이터 전처리 


```python
x_train, x_test = x_train/255.0, x_test/255.0
```

### 모델 구성


```python
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)), #1차원 배열로 설정
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
```

* 활성화 함수 activation  
  
신경망 모델의 각 층에 input값과 w,b를 곱/합 연산을 통해 a=wx+b를 계산하고 마지막에  
활성화 함수를 거쳐 a를 출력. 각 층마다 활성화 함수를 사용  
  
xor 문제 해결할 수 없어서 나온 개념이 은닉층(hidden layer)인데,  
무작정 쌓기만 한다고 선형분류를 비선형으로 바꿀 수 있는 것이 아니기 때문  
  
그래서 나온 해결책이 **활성화 함수**  
활성화 함수를 사용하면 입력값에 대한 출력값이 linear하게 나오지 않으므로  
선형분류기를 비선형 시스템으로 만들 수 있음

### 모델 컴파일
손실함수를 줄이기 위해 사용되는 과정


```python
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

### 모델 훈련


```python
model.fit(x_train, y_train, epochs=5)
```

    Epoch 1/5
    1875/1875 [==============================] - 31s 15ms/step - loss: 0.2006 - accuracy: 0.94040s - l
    Epoch 2/5
    1875/1875 [==============================] - 23s 12ms/step - loss: 0.0810 - accuracy: 0.9742
    Epoch 3/5
    1875/1875 [==============================] - 22s 12ms/step - loss: 0.0524 - accuracy: 0.9835
    Epoch 4/5
    1875/1875 [==============================] - 26s 14ms/step - loss: 0.0378 - accuracy: 0.9879
    Epoch 5/5
    1875/1875 [==============================] - 21s 11ms/step - loss: 0.0265 - accuracy: 0.9912
    




    <tensorflow.python.keras.callbacks.History at 0x22a8f05e670>



### 정확도 평가 


```python
test_loss, test_acc = model.evaluate(x_test, y_test)
print('테스트 정확도:', test_acc)
```

    313/313 [==============================] - 4s 7ms/step - loss: 0.0685 - accuracy: 0.9809
    테스트 정확도: 0.98089998960495
    

#### 손실값과 정확도 확인하기 


```python
loss, accuracy = [], []
for i in range(10):
    model.fit(x_train, y_train, epochs=1)
    loss.append(model.evaluate(x_test, y_test)[0])
    accuracy.append(model.evaluate(x_test, y_test)[1])

print(accuracy)
```

    1875/1875 [==============================] - 23s 12ms/step - loss: 0.0207 - accuracy: 0.9930
    313/313 [==============================] - 2s 7ms/step - loss: 0.0753 - accuracy: 0.9806
    313/313 [==============================] - 2s 8ms/step - loss: 0.0753 - accuracy: 0.9806
    1875/1875 [==============================] - 21s 11ms/step - loss: 0.0180 - accuracy: 0.9938
    313/313 [==============================] - 2s 6ms/step - loss: 0.0865 - accuracy: 0.9786
    313/313 [==============================] - 2s 7ms/step - loss: 0.0865 - accuracy: 0.9786
    1875/1875 [==============================] - 23s 12ms/step - loss: 0.0145 - accuracy: 0.9953
    313/313 [==============================] - 2s 6ms/step - loss: 0.0791 - accuracy: 0.9799
    313/313 [==============================] - 2s 6ms/step - loss: 0.0791 - accuracy: 0.9799
    1875/1875 [==============================] - 25s 13ms/step - loss: 0.0112 - accuracy: 0.9965
    313/313 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.98 - 2s 6ms/step - loss: 0.0795 - accuracy: 0.9816
    313/313 [==============================] - 2s 7ms/step - loss: 0.0795 - accuracy: 0.9816
    1875/1875 [==============================] - 27s 15ms/step - loss: 0.0121 - accuracy: 0.9962
    313/313 [==============================] - 3s 10ms/step - loss: 0.0851 - accuracy: 0.9801 0s - loss:
    313/313 [==============================] - 2s 7ms/step - loss: 0.0851 - accuracy: 0.9801
    1875/1875 [==============================] - 25s 14ms/step - loss: 0.0103 - accuracy: 0.9965
    313/313 [==============================] - 2s 6ms/step - loss: 0.0683 - accuracy: 0.9834
    313/313 [==============================] - 2s 6ms/step - loss: 0.0683 - accuracy: 0.9834
    1875/1875 [==============================] - 21s 11ms/step - loss: 0.0082 - accuracy: 0.9976
    313/313 [==============================] - 2s 7ms/step - loss: 0.0879 - accuracy: 0.9802
    313/313 [==============================] - 2s 7ms/step - loss: 0.0879 - accuracy: 0.9802
    1875/1875 [==============================] - 28s 15ms/step - loss: 0.0105 - accuracy: 0.9964
    313/313 [==============================] - 3s 9ms/step - loss: 0.0889 - accuracy: 0.9816
    313/313 [==============================] - 2s 7ms/step - loss: 0.0889 - accuracy: 0.9816
    1875/1875 [==============================] - 20s 11ms/step - loss: 0.0053 - accuracy: 0.9982
    313/313 [==============================] - 3s 8ms/step - loss: 0.1153 - accuracy: 0.9787
    313/313 [==============================] - 2s 6ms/step - loss: 0.1153 - accuracy: 0.9787
    1875/1875 [==============================] - 21s 11ms/step - loss: 0.0084 - accuracy: 0.9975
    313/313 [==============================] - 2s 7ms/step - loss: 0.1045 - accuracy: 0.9819
    313/313 [==============================] - 2s 7ms/step - loss: 0.1045 - accuracy: 0.9819
    [0.9805999994277954, 0.978600025177002, 0.9799000024795532, 0.9815999865531921, 0.9800999760627747, 0.9833999872207642, 0.9801999926567078, 0.9815999865531921, 0.9786999821662903, 0.9818999767303467]
    
