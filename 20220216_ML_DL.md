```python
import pandas as pd
import numpy as np

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
  </tbody>
</table>
</div>



### k최근접 이웃의 다중분류


```python
fish_input= fish[['Weight','Length','Diagonal','Height','Width']].to_numpy() #리스트로 필요한 열만 추출해서 넣을 수 0
fish_target = fish['Species'].to_numpy()
```


```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```


```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)
```




    KNeighborsClassifier(n_neighbors=3)




```python
print(kn.classes_) #특성확인 
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    

**0번째 행부터 4번째 행까지 test데이터를 통해 아래와 같은 생선으로 본 것**


```python
print(kn.predict(test_scaled[:5])) # 5개의 샘플 뽑아서 확인 
```

    ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
    

**예측한 것들에 대해 수치로 나타낸 것** 


```python
proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba,decimals=3))
```

    [[0.    0.    1.    0.    0.    0.    0.   ]
     [0.    0.    0.    0.    0.    1.    0.   ]
     [0.    0.    0.    1.    0.    0.    0.   ]
     [0.    0.    0.667 0.    0.333 0.    0.   ]
     [0.    0.    0.667 0.    0.333 0.    0.   ]]
    

### 로지스틱 회귀로 이진 분류 수행


```python
#불리안 인덱싱 사용 - bream과 smelt만 가져오기 
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt') 

train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```


```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```




    LogisticRegression()




```python
print(lr.predict(train_bream_smelt[:5]))
```

    ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
    


```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

    [[0.99759855 0.00240145]
     [0.02735183 0.97264817]
     [0.99486072 0.00513928]
     [0.98584202 0.01415798]
     [0.99767269 0.00232731]]
    


```python
print(lr.coef_, lr.intercept_) #5개의 특성 사용했으므로 5개의 기울기 나옴
```

    [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
    


```python
fish.head(0) 
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
</div>



z = -0.4037798*무게 -0.57620209*길이 -0.66280298*대각선 -1.01290277*높이  -0.73168947*두께 - 2.16155


```python
decisions = lr.decision_function(train_bream_smelt[:5]) #z값을 출력해줌 
print(decisions)
```

    [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
    

#### 시그모이드 함수


```python
import numpy as np
import matplotlib.pyplot as plt

z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))

plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()
```


    
![png](output_21_0.png)
    


시그모이드 함수는 로지스틱함수가 대표적으로 있으며  
0 - 1사이의 값을 갖고 x축을 볼때 음수를 나타내면 음성  
0이상의 값을 갖으면 양성을 나타냄  


```python
from scipy.special import expit #시그모이드 함수 라이브러리

print(expit(decisions))
```

    [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
    


```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

    [[0.99759855 0.00240145]
     [0.02735183 0.97264817]
     [0.99486072 0.00513928]
     [0.98584202 0.01415798]
     [0.99767269 0.00232731]]
    

빙어 예측값과 같은 결과를 보여주고 있다.  
현재 2진분류로 둘을 나타내었을 때 빙어를 양성, 도미를 음성   
시그모이드 함수를 이용할 경우엔 양성이라 생각한 빙어만을 보여줌

### 로지스틱 회귀로 다중 분류 


```python
lr = LogisticRegression(C=20, max_iter=1000)
# max_iter는 반복값 C는 규제정도 - 값이 작아지면 규제 강해지고 커지면 약해짐

lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

    0.9327731092436975
    0.925
    


```python
print(lr.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    


```python
print(lr.predict(test_scaled[:5]))                                                                                      
```

    ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
    


```python
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    


```python
print(lr.coef_.shape, lr.intercept_.shape)
```

    (7, 5) (7,)
    

(7,5)는 7개의 클래스를 가지며 5개는 특성과 곱해지는 계수   
클래스마다 선형함수가 생성됨 즉, z값이 클래스마다 생성  

### 소프트맥스 함수

시그모이드 함수가 이진분류에서 사용한다면,  
소프트맥스는 다중분류에서 사용된다.  



```python
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))
```

    [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
     [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
     [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
     [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
     [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
    


```python
from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    

로지스틱 회귀는 선형회귀와 비슷하게 학습해서 결과 확인하지만,  
결과값을 그대로 사용하지 않고 이진분류면 시그모이드 함수 사용  
다중분류는 소프트맥스 함수를 사용하여 결과값을 확인한다.  

