### ë‹¨ì–´ ê¸°ë°˜ ì¸ì½”ë”©


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
  'I love my dog',
  'I love my cat'
]

tokenizer = Tokenizer(num_words = 100) #ë‹¨ì–´ ê°œìˆ˜ ì œí•œ 
tokenizer.fit_on_texts(sentences)#ë¬¸ì ë°ì´í„°ë¥¼ ì…ë ¥ ë°›ì•„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ 
word_index = tokenizer.word_index
print(word_index)#í‚¤ ê°’ ìŒìœ¼ë¡œ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ 
```

    {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}
    


```python
sentences = [
  'I love my dog',
  'I love my cat',
  'You love my dog!'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)
```

    {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}
    

**!**ê°€ ë¹ ì§€ê³  dogëŠ” ì›ë˜ ìˆë˜ dogë¡œ ë°˜ì˜ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

### í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ê¸°


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
  'I love my dog',
  'I love my cat',
  'You love my dog!',
  'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)
#texts_to_sequences() í…ìŠ¤íŠ¸ ì•ˆ ë‹¨ì–´ë“¤ì„ ìˆ«ìì˜ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë°˜í™˜ 

print(word_index)
print(sequences)
```

    {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}
    [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]
    


```python
test_sentences = [
  'i really love my dog',
  'my dog loves my friend'
]

test_sequences = tokenizer.texts_to_sequences(test_sentences)
print(test_sequences)
```

    [[4, 2, 1, 3], [1, 3, 1]]
    

ìƒˆë¡œìš´ ë¬¸ì¥ì€ tokenizerë¥¼ í•˜ì§€ ì•Šì•„ í¬í•¨ë˜ì§€ ì•Šì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

### í† í°í™”ë˜ì§€ ì•Šì€ ë‹¨ì–´ ì²˜ë¦¬í•˜ê¸° 

* í† í°í™” : ë¬¸ì¥ì„ ìµœì†Œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì˜ë¼ ì»´í“¨í„°ê°€ ì¸ì‹í•˜ë„ë¡ ë•ëŠ” ë°©ë²•  
  ë¬¸ì¥ í˜•íƒœì˜ ë°ì´í„° ì „ì²˜ë¦¬í•  ë•Œ ë§ì´ ì‚¬ìš©í•˜ë©°, ë§ë­‰ì¹˜ë¥¼ ì–´ë–¤ í† í°ì˜ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëƒì— ë”°ë¼  ì§‘í•©ì˜ í¬ê¸° , ë‹¨ì–´ ì§‘í•©ì´ í‘œí˜„í•˜ëŠ” í† í¬ì˜ í˜•íƒœê°€ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚¨.  ì´ë•Œ í…ìŠ¤íŠ¸ë¥¼ í† í°ì˜ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…ì„ ëœ»í•¨


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>")
# oov_token - ì¸ë±ì‹±í•˜ì§€ ì•Šì€ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œ ì‚¬ìš© 
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)

test_sentences = [
    'i really love my dog',
    'my dog loves my friend'
]

test_sequences = tokenizer.texts_to_sequences(test_sentences)
print(test_sequences)
print(word_index)
```

    [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]
    {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}
    

### íŒ¨ë”© ì„¤ì •í•˜ê¸° 
ì„œë¡œ ë‹¤ë¥¸ ê°œìˆ˜ì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì„ ê°™ì€ ê¸¸ì´ë¡œ ë§Œë“¤ê¸° ìœ„í•´ íŒ¨ë”© ì‚¬ìš©


```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
```


```python
sentences = [
  'I love my dog',
  'I love my cat',
  'You love my dog!',
  'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences)
#íŒ¨ë”©ì„ í•˜ê¸° ìœ„í•´ pad_sequncesí•¨ìˆ˜ë¥¼ ì‚¬ìš© 

print(word_index)
print(sequences)
print(padded)
```

    {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}
    [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]
    [[ 0  0  0  5  3  2  4]
     [ 0  0  0  5  3  2  7]
     [ 0  0  0  6  3  2  4]
     [ 8  6  9  2  4 10 11]]
    

íŒ¨ë”©ì„ í•˜ë©´ ìˆ«ì 0ì„ ì´ìš©í•´ ê°™ì€ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•´ì¤Œ 

#### padding íŒŒë¼ë¯¸í„°
padding íŒŒë¼ë¯¸í„°ë¥¼ postë¡œ ì„¤ì •í•˜ê²Œ ë˜ë©´ ì‹œí€€ìŠ¤ì˜ ë’¤ì— 0ìœ¼ë¡œ ì±„ì›Œì§  
ë””í´íŠ¸ ê°’ì€ preë¡œ ì•ì˜ ê²°ê³¼ì™€ ê°™ì´ ì•ì— 0 ì´ ì±„ì›Œì§


```python
padded = pad_sequences(sequences, padding='post')

print(padded)
```

    [[ 5  3  2  4  0  0  0]
     [ 5  3  2  7  0  0  0]
     [ 6  3  2  4  0  0  0]
     [ 8  6  9  2  4 10 11]]
    

#### maxlen íŒŒë¼ë¯¸í„°
íŒ¨ë”©ì˜ ìµœëŒ€ ê¸¸ì´ ì„¤ì • 


```python
padded = pad_sequences(sequences, padding='pre', maxlen=6)

print(padded)
```

    [[ 0  0  5  3  2  4]
     [ 0  0  5  3  2  7]
     [ 0  0  6  3  2  4]
     [ 6  9  2  4 10 11]]
    

#### truncating íŒŒë¼ë¯¸í„°
ìµœëŒ€ ê¸¸ì´ ë„˜ëŠ” ì‹œí€€ìŠ¤ ì˜ë¼ë‚¼ ìœ„ì¹˜ ì§€ì •


```python
padded = pad_sequences(sequences, padding='pre', maxlen=6, truncating='post')

print(padded)
```

    [[ 0  0  5  3  2  4]
     [ 0  0  5  3  2  7]
     [ 0  0  6  3  2  4]
     [ 8  6  9  2  4 10]]
    

## ë°ì´í„° ì¤€ë¹„í•˜ê¸°


```python
# !pip install -q tensorflow-datasets
```


```python
import tensorflow_datasets as tfds
imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
```

    [1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\juhee\tensorflow_datasets\imdb_reviews\plain_text\1.0.0...[0m
    


    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progreâ€¦



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressStyâ€¦


    
    
    


    HBox(children=(FloatProgress(value=0.0, description='Generating splits...', max=3.0, style=ProgressStyle(descrâ€¦



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating train examples...', max=1.0,â€¦



    HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\Users\\juhee\\tensorflow_datasets\\imdb_revâ€¦



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating test examples...', max=1.0, â€¦



    HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\Users\\juhee\\tensorflow_datasets\\imdb_revâ€¦



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating unsupervised examples...', mâ€¦



    HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\Users\\juhee\\tensorflow_datasets\\imdb_revâ€¦


    [1mDataset imdb_reviews downloaded and prepared to C:\Users\juhee\tensorflow_datasets\imdb_reviews\plain_text\1.0.0. Subsequent calls will reuse this data.[0m
    

### ë°ì´í„° ì‚´í´ë³´ê¸°

IMDB ë°ì´í„°ëŠ” í…ìŠ¤íŠ¸ë¡œë¶€í„° ê°ì • ë¶„ì„ ì‹¤ì‹œí•˜ëŠ” ë°ì´í„°  
ê¸ì • / ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜ëœ 50,000ê°œì˜ ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ í¬í•¨  
25,000ê°œ í›ˆë ¨ ë°ì´í„°ì™€ 25,000ê°œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ êµ¬ì„±


```python
#ë¦¬ë·°ê°€ ê¸ì •ì´ë©´ 1 / ë¶€ì •ì´ë©´ 0ìœ¼ë¡œ ë ˆì´ë¸” ì„¤ì •ë˜ì–´ ìˆìŒ

import numpy as np

train_data, test_data = imdb['train'], imdb['test']
train_sentences = []
train_labels = []
test_sentences = []
test_labels = []

for s, l in train_data:
  train_sentences.append(str(s.numpy()))
  train_labels.append(l.numpy())

for s, l in test_data:
  test_sentences.append(str(s.numpy()))
  test_labels.append(l.numpy())

print(train_sentences[0])
print(train_labels[0])
print(test_sentences[0])
print(test_labels[0])
```

    b"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it."
    0
    b"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come."
    1
    


```python
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)
```


```python
print(train_labels[:10])
print(test_labels[:10])
```

    [0 0 0 1 1 1 0 0 0 0]
    [1 1 0 0 1 1 1 1 0 1]
    

### ë¦¬ë·° ë¬¸ì¥ í† í°í™”í•˜ê¸°
1. í† í°í™”í•  ë‹¨ì–´ì˜ ìˆ˜ ë° ì´ì™¸ì˜ íŒŒë¼ë¯¸í„° ì„¤ì •  
2. tokenizer ì™€ pad_sequences()í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°  
3. fit_on_textsì´ìš©í•´ ë‹¨ì–´ í† í°í™”í•˜ê³  texts_to_sequencesì´ìš©í•´ ìˆ«ì ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
4. pad_sequences ì‚¬ìš©í•´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •


```python
vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = "<OOV>"

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(train_sentences)
padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)

test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences, maxlen=max_length)

print(sequences[0])
print(padded[0])

print(test_sequences[0])
print(test_padded[0])
```

    [59, 12, 14, 35, 439, 400, 18, 174, 29, 1, 9, 33, 1378, 3401, 42, 496, 1, 197, 25, 88, 156, 19, 12, 211, 340, 29, 70, 248, 213, 9, 486, 62, 70, 88, 116, 99, 24, 5740, 12, 3317, 657, 777, 12, 18, 7, 35, 406, 8228, 178, 2477, 426, 2, 92, 1253, 140, 72, 149, 55, 2, 1, 7525, 72, 229, 70, 2962, 16, 1, 2880, 1, 1, 1506, 4998, 3, 40, 3947, 119, 1608, 17, 3401, 14, 163, 19, 4, 1253, 927, 7986, 9, 4, 18, 13, 14, 4200, 5, 102, 148, 1237, 11, 240, 692, 13, 44, 25, 101, 39, 12, 7232, 1, 39, 1378, 1, 52, 409, 11, 99, 1214, 874, 145, 10]
    [   0    0   59   12   14   35  439  400   18  174   29    1    9   33
     1378 3401   42  496    1  197   25   88  156   19   12  211  340   29
       70  248  213    9  486   62   70   88  116   99   24 5740   12 3317
      657  777   12   18    7   35  406 8228  178 2477  426    2   92 1253
      140   72  149   55    2    1 7525   72  229   70 2962   16    1 2880
        1    1 1506 4998    3   40 3947  119 1608   17 3401   14  163   19
        4 1253  927 7986    9    4   18   13   14 4200    5  102  148 1237
       11  240  692   13   44   25  101   39   12 7232    1   39 1378    1
       52  409   11   99 1214  874  145   10]
    [59, 44, 25, 109, 13, 97, 4115, 16, 742, 4370, 10, 14, 316, 5, 2, 593, 354, 16, 1864, 1212, 1, 16, 680, 7499, 5595, 1, 773, 6, 13, 1037, 1, 1, 439, 491, 1, 4, 1, 334, 3610, 20, 229, 3, 15, 5796, 3, 15, 1646, 15, 102, 5, 2, 3597, 101, 11, 1450, 1528, 12, 251, 235, 11, 216, 2, 377, 6429, 3, 62, 95, 11, 174, 105, 11, 1528, 180, 12, 251, 37, 6, 1144, 1, 682, 7, 4452, 1, 4, 1, 334, 7, 37, 8367, 377, 5, 1420, 1, 13, 30, 64, 28, 6, 874, 181, 17, 4, 1050, 5, 12, 224, 3, 83, 4, 353, 33, 353, 5229, 5, 10, 6, 1340, 1160, 2, 5738, 1, 3, 1, 5, 10, 175, 328, 7, 1319, 3989, 4, 798, 1946, 5, 4, 250, 2710, 158, 3, 2, 361, 31, 187, 25, 1170, 499, 610, 5, 2, 122, 2, 356, 1398, 7725, 30, 1, 881, 38, 4, 20, 39, 12, 1, 4, 1, 334, 7, 4, 20, 634, 60, 48, 214]
    [  11 1450 1528   12  251  235   11  216    2  377 6429    3   62   95
       11  174  105   11 1528  180   12  251   37    6 1144    1  682    7
     4452    1    4    1  334    7   37 8367  377    5 1420    1   13   30
       64   28    6  874  181   17    4 1050    5   12  224    3   83    4
      353   33  353 5229    5   10    6 1340 1160    2 5738    1    3    1
        5   10  175  328    7 1319 3989    4  798 1946    5    4  250 2710
      158    3    2  361   31  187   25 1170  499  610    5    2  122    2
      356 1398 7725   30    1  881   38    4   20   39   12    1    4    1
      334    7    4   20  634   60   48  214]
    

### ëª¨ë¸ êµ¬ì„±í•˜ê¸°
í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ì˜ í•µì‹¬ì ì¸ ë¶€ë¶„ì€ **Embedding**  
ì„ë² ë”©ì˜ ê²°ê³¼ëŠ” (vocab_size, embedding_dim)ì˜ í˜•íƒœë¥¼ ê°–ëŠ” 2ì°¨ì› ë°°ì—´ í˜•íƒœê°€ê°€ ë˜ê³ , ì´ë¯¸ì§€ ë¶„ë¥˜ì™€ ë§ˆì°¬ê°€ì§€ë¡œ flattenì´ìš©í•´ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜

* Embedding : íŠ¹ì§• ì¶”ì¶œì„ í†µí•´ ìˆ˜ì¹˜í™” í•´ì¤˜ì•¼í•˜ëŠ” ë° ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒ


```python
import tensorflow as tf

model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(6, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding (Embedding)        (None, 120, 16)           160000    
    _________________________________________________________________
    flatten (Flatten)            (None, 1920)              0         
    _________________________________________________________________
    dense (Dense)                (None, 6)                 11526     
    _________________________________________________________________
    dense_1 (Dense)              (None, 1)                 7         
    =================================================================
    Total params: 171,533
    Trainable params: 171,533
    Non-trainable params: 0
    _________________________________________________________________
    

## ëª¨ë¸ ì»´íŒŒì¼


```python
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```

## ëª¨ë¸ í›ˆë ¨í•˜ê¸°


```python
num_epochs = 10
model.fit(padded, train_labels, epochs=num_epochs,
        validation_data=(test_padded, test_labels))
```

    Epoch 1/10
    782/782 [==============================] - 14s 12ms/step - loss: 0.4925 - accuracy: 0.7434 - val_loss: 0.3502 - val_accuracy: 0.8453
    Epoch 2/10
    782/782 [==============================] - 8s 10ms/step - loss: 0.2396 - accuracy: 0.9070 - val_loss: 0.3707 - val_accuracy: 0.8354
    Epoch 3/10
    782/782 [==============================] - 7s 9ms/step - loss: 0.0869 - accuracy: 0.9796 - val_loss: 0.4561 - val_accuracy: 0.8253
    Epoch 4/10
    782/782 [==============================] - 7s 9ms/step - loss: 0.0226 - accuracy: 0.9970 - val_loss: 0.5365 - val_accuracy: 0.8262
    Epoch 5/10
    782/782 [==============================] - 7s 10ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.5939 - val_accuracy: 0.8238
    Epoch 6/10
    782/782 [==============================] - 7s 9ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6434 - val_accuracy: 0.8263
    Epoch 7/10
    782/782 [==============================] - 7s 9ms/step - loss: 8.3113e-04 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.8272
    Epoch 8/10
    782/782 [==============================] - 7s 9ms/step - loss: 4.6895e-04 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 0.8276
    Epoch 9/10
    782/782 [==============================] - 8s 10ms/step - loss: 2.6739e-04 - accuracy: 1.0000 - val_loss: 0.7653 - val_accuracy: 0.8270
    Epoch 10/10
    782/782 [==============================] - 7s 9ms/step - loss: 1.6052e-04 - accuracy: 1.0000 - val_loss: 0.8025 - val_accuracy: 0.8280
    




    <tensorflow.python.keras.callbacks.History at 0x1fb8eab29d0>



í›ˆë ¨ë°ì´í„°ì— ëŒ€í•´ 1.0ì˜ ì •í™•ë„, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ 0.8280ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤Œ
