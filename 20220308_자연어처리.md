### 단어 기반 인코딩


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
  'I love my dog',
  'I love my cat'
]

tokenizer = Tokenizer(num_words = 100) #단어 개수 제한 
tokenizer.fit_on_texts(sentences)#문자 데이터를 입력 받아 리스트 형태로 변환 
word_index = tokenizer.word_index
print(word_index)#키 값 쌍으로 포함하는 딕셔너리 반환 
```

    {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}
    


```python
sentences = [
  'I love my dog',
  'I love my cat',
  'You love my dog!'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)
```

    {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}
    

**!**가 빠지고 dog는 원래 있던 dog로 반영된 것을 볼 수 있음

### 텍스트를 시퀀스로 변환하기


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
  'I love my dog',
  'I love my cat',
  'You love my dog!',
  'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)
#texts_to_sequences() 텍스트 안 단어들을 숫자의 시퀀스 형태로 반환 

print(word_index)
print(sequences)
```

    {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}
    [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]
    


```python
test_sentences = [
  'i really love my dog',
  'my dog loves my friend'
]

test_sequences = tokenizer.texts_to_sequences(test_sentences)
print(test_sequences)
```

    [[4, 2, 1, 3], [1, 3, 1]]
    

새로운 문장은 tokenizer를 하지 않아 포함되지 않은 것을 확인할 수 있음

### 토큰화되지 않은 단어 처리하기 

* 토큰화 : 문장을 최소 의미 단위로 잘라 컴퓨터가 인식하도록 돕는 방법  
  문장 형태의 데이터 전처리할 때 많이 사용하며, 말뭉치를 어떤 토큰의 단위로 분할하냐에 따라  집합의 크기 , 단어 집합이 표현하는 토크의 형태가 다르게 나타남.  이때 텍스트를 토큰의 단위로 분할하는 작업을 뜻함


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>")
# oov_token - 인덱싱하지 않은 단어를 나타내기 위해서 사용 
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)

test_sentences = [
    'i really love my dog',
    'my dog loves my friend'
]

test_sequences = tokenizer.texts_to_sequences(test_sentences)
print(test_sequences)
print(word_index)
```

    [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]
    {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}
    

### 패딩 설정하기 
서로 다른 개수의 단어로 이루어진 문장을 같은 길이로 만들기 위해 패딩 사용


```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
```


```python
sentences = [
  'I love my dog',
  'I love my cat',
  'You love my dog!',
  'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences)
#패딩을 하기 위해 pad_sequnces함수를 사용 

print(word_index)
print(sequences)
print(padded)
```

    {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}
    [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]
    [[ 0  0  0  5  3  2  4]
     [ 0  0  0  5  3  2  7]
     [ 0  0  0  6  3  2  4]
     [ 8  6  9  2  4 10 11]]
    

패딩을 하면 숫자 0을 이용해 같은 길이의 시퀀스로 변환해줌 

#### padding 파라미터
padding 파라미터를 post로 설정하게 되면 시퀀스의 뒤에 0으로 채워짐  
디폴트 값은 pre로 앞의 결과와 같이 앞에 0 이 채워짐


```python
padded = pad_sequences(sequences, padding='post')

print(padded)
```

    [[ 5  3  2  4  0  0  0]
     [ 5  3  2  7  0  0  0]
     [ 6  3  2  4  0  0  0]
     [ 8  6  9  2  4 10 11]]
    

#### maxlen 파라미터
패딩의 최대 길이 설정 


```python
padded = pad_sequences(sequences, padding='pre', maxlen=6)

print(padded)
```

    [[ 0  0  5  3  2  4]
     [ 0  0  5  3  2  7]
     [ 0  0  6  3  2  4]
     [ 6  9  2  4 10 11]]
    

#### truncating 파라미터
최대 길이 넘는 시퀀스 잘라낼 위치 지정


```python
padded = pad_sequences(sequences, padding='pre', maxlen=6, truncating='post')

print(padded)
```

    [[ 0  0  5  3  2  4]
     [ 0  0  5  3  2  7]
     [ 0  0  6  3  2  4]
     [ 8  6  9  2  4 10]]
    

## 데이터 준비하기


```python
# !pip install -q tensorflow-datasets
```


```python
import tensorflow_datasets as tfds
imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
```

    [1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\juhee\tensorflow_datasets\imdb_reviews\plain_text\1.0.0...[0m
    


    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…


    
    
    


    HBox(children=(FloatProgress(value=0.0, description='Generating splits...', max=3.0, style=ProgressStyle(descr…



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating train examples...', max=1.0,…



    HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\Users\\juhee\\tensorflow_datasets\\imdb_rev…



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating test examples...', max=1.0, …



    HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\Users\\juhee\\tensorflow_datasets\\imdb_rev…



    HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating unsupervised examples...', m…



    HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\Users\\juhee\\tensorflow_datasets\\imdb_rev…


    [1mDataset imdb_reviews downloaded and prepared to C:\Users\juhee\tensorflow_datasets\imdb_reviews\plain_text\1.0.0. Subsequent calls will reuse this data.[0m
    

### 데이터 살펴보기

IMDB 데이터는 텍스트로부터 감정 분석 실시하는 데이터  
긍정 / 부정으로 분류된 50,000개의 영화 리뷰 텍스트 포함  
25,000개 훈련 데이터와 25,000개의 테스트 데이터로 구성


```python
#리뷰가 긍정이면 1 / 부정이면 0으로 레이블 설정되어 있음

import numpy as np

train_data, test_data = imdb['train'], imdb['test']
train_sentences = []
train_labels = []
test_sentences = []
test_labels = []

for s, l in train_data:
  train_sentences.append(str(s.numpy()))
  train_labels.append(l.numpy())

for s, l in test_data:
  test_sentences.append(str(s.numpy()))
  test_labels.append(l.numpy())

print(train_sentences[0])
print(train_labels[0])
print(test_sentences[0])
print(test_labels[0])
```

    b"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it."
    0
    b"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come."
    1
    


```python
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)
```


```python
print(train_labels[:10])
print(test_labels[:10])
```

    [0 0 0 1 1 1 0 0 0 0]
    [1 1 0 0 1 1 1 1 0 1]
    

### 리뷰 문장 토큰화하기
1. 토큰화할 단어의 수 및 이외의 파라미터 설정  
2. tokenizer 와 pad_sequences()함수 불러오기  
3. fit_on_texts이용해 단어 토큰화하고 texts_to_sequences이용해 숫자 시퀀스로 변환
4. pad_sequences 사용해 시퀀스 길이 설정


```python
vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = "<OOV>"

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(train_sentences)
padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)

test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences, maxlen=max_length)

print(sequences[0])
print(padded[0])

print(test_sequences[0])
print(test_padded[0])
```

    [59, 12, 14, 35, 439, 400, 18, 174, 29, 1, 9, 33, 1378, 3401, 42, 496, 1, 197, 25, 88, 156, 19, 12, 211, 340, 29, 70, 248, 213, 9, 486, 62, 70, 88, 116, 99, 24, 5740, 12, 3317, 657, 777, 12, 18, 7, 35, 406, 8228, 178, 2477, 426, 2, 92, 1253, 140, 72, 149, 55, 2, 1, 7525, 72, 229, 70, 2962, 16, 1, 2880, 1, 1, 1506, 4998, 3, 40, 3947, 119, 1608, 17, 3401, 14, 163, 19, 4, 1253, 927, 7986, 9, 4, 18, 13, 14, 4200, 5, 102, 148, 1237, 11, 240, 692, 13, 44, 25, 101, 39, 12, 7232, 1, 39, 1378, 1, 52, 409, 11, 99, 1214, 874, 145, 10]
    [   0    0   59   12   14   35  439  400   18  174   29    1    9   33
     1378 3401   42  496    1  197   25   88  156   19   12  211  340   29
       70  248  213    9  486   62   70   88  116   99   24 5740   12 3317
      657  777   12   18    7   35  406 8228  178 2477  426    2   92 1253
      140   72  149   55    2    1 7525   72  229   70 2962   16    1 2880
        1    1 1506 4998    3   40 3947  119 1608   17 3401   14  163   19
        4 1253  927 7986    9    4   18   13   14 4200    5  102  148 1237
       11  240  692   13   44   25  101   39   12 7232    1   39 1378    1
       52  409   11   99 1214  874  145   10]
    [59, 44, 25, 109, 13, 97, 4115, 16, 742, 4370, 10, 14, 316, 5, 2, 593, 354, 16, 1864, 1212, 1, 16, 680, 7499, 5595, 1, 773, 6, 13, 1037, 1, 1, 439, 491, 1, 4, 1, 334, 3610, 20, 229, 3, 15, 5796, 3, 15, 1646, 15, 102, 5, 2, 3597, 101, 11, 1450, 1528, 12, 251, 235, 11, 216, 2, 377, 6429, 3, 62, 95, 11, 174, 105, 11, 1528, 180, 12, 251, 37, 6, 1144, 1, 682, 7, 4452, 1, 4, 1, 334, 7, 37, 8367, 377, 5, 1420, 1, 13, 30, 64, 28, 6, 874, 181, 17, 4, 1050, 5, 12, 224, 3, 83, 4, 353, 33, 353, 5229, 5, 10, 6, 1340, 1160, 2, 5738, 1, 3, 1, 5, 10, 175, 328, 7, 1319, 3989, 4, 798, 1946, 5, 4, 250, 2710, 158, 3, 2, 361, 31, 187, 25, 1170, 499, 610, 5, 2, 122, 2, 356, 1398, 7725, 30, 1, 881, 38, 4, 20, 39, 12, 1, 4, 1, 334, 7, 4, 20, 634, 60, 48, 214]
    [  11 1450 1528   12  251  235   11  216    2  377 6429    3   62   95
       11  174  105   11 1528  180   12  251   37    6 1144    1  682    7
     4452    1    4    1  334    7   37 8367  377    5 1420    1   13   30
       64   28    6  874  181   17    4 1050    5   12  224    3   83    4
      353   33  353 5229    5   10    6 1340 1160    2 5738    1    3    1
        5   10  175  328    7 1319 3989    4  798 1946    5    4  250 2710
      158    3    2  361   31  187   25 1170  499  610    5    2  122    2
      356 1398 7725   30    1  881   38    4   20   39   12    1    4    1
      334    7    4   20  634   60   48  214]
    

### 모델 구성하기
텍스트 감정 분석의 핵심적인 부분은 **Embedding**  
임베딩의 결과는 (vocab_size, embedding_dim)의 형태를 갖는 2차원 배열 형태가가 되고, 이미지 분류와 마찬가지로 flatten이용해 1차원으로 변환

* Embedding : 특징 추출을 통해 수치화 해줘야하는 데 이때 사용하는 것


```python
import tensorflow as tf

model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(6, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding (Embedding)        (None, 120, 16)           160000    
    _________________________________________________________________
    flatten (Flatten)            (None, 1920)              0         
    _________________________________________________________________
    dense (Dense)                (None, 6)                 11526     
    _________________________________________________________________
    dense_1 (Dense)              (None, 1)                 7         
    =================================================================
    Total params: 171,533
    Trainable params: 171,533
    Non-trainable params: 0
    _________________________________________________________________
    

## 모델 컴파일


```python
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```

## 모델 훈련하기


```python
num_epochs = 10
model.fit(padded, train_labels, epochs=num_epochs,
        validation_data=(test_padded, test_labels))
```

    Epoch 1/10
    782/782 [==============================] - 14s 12ms/step - loss: 0.4925 - accuracy: 0.7434 - val_loss: 0.3502 - val_accuracy: 0.8453
    Epoch 2/10
    782/782 [==============================] - 8s 10ms/step - loss: 0.2396 - accuracy: 0.9070 - val_loss: 0.3707 - val_accuracy: 0.8354
    Epoch 3/10
    782/782 [==============================] - 7s 9ms/step - loss: 0.0869 - accuracy: 0.9796 - val_loss: 0.4561 - val_accuracy: 0.8253
    Epoch 4/10
    782/782 [==============================] - 7s 9ms/step - loss: 0.0226 - accuracy: 0.9970 - val_loss: 0.5365 - val_accuracy: 0.8262
    Epoch 5/10
    782/782 [==============================] - 7s 10ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.5939 - val_accuracy: 0.8238
    Epoch 6/10
    782/782 [==============================] - 7s 9ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6434 - val_accuracy: 0.8263
    Epoch 7/10
    782/782 [==============================] - 7s 9ms/step - loss: 8.3113e-04 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.8272
    Epoch 8/10
    782/782 [==============================] - 7s 9ms/step - loss: 4.6895e-04 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 0.8276
    Epoch 9/10
    782/782 [==============================] - 8s 10ms/step - loss: 2.6739e-04 - accuracy: 1.0000 - val_loss: 0.7653 - val_accuracy: 0.8270
    Epoch 10/10
    782/782 [==============================] - 7s 9ms/step - loss: 1.6052e-04 - accuracy: 1.0000 - val_loss: 0.8025 - val_accuracy: 0.8280
    




    <tensorflow.python.keras.callbacks.History at 0x1fb8eab29d0>



훈련데이터에 대해 1.0의 정확도, 테스트 데이터에 대해 0.8280의 정확도를 보여줌
